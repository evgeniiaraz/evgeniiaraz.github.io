---
layout: page
title: Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond
comments: false
tags: [blog, Evgeniia-Razumovskaia, research, NLP, dialogue systems, welcome, phd-life, phd, LASER, multilingual sentence embeddings]
---

These notes are the second series of my retrospective of multilingual sentence encoders. You can find previous one <a href="https://evgeniiaraz.github.io/blog/posts/muse/">here</a>.
<br/>
<br/>

<span style="font-size:14px;">
The notes are on the paper
    <b><a href="https://arxiv.org/pdf/1812.10464.pdf">
        Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond
    </a></b>
    by Mikel Artetxe and Holger Schwenk.
</span>
<br/>
<br/>

<span style="font-size:15px;"><b>Preface</b></span>
<br/>

  🔲    The overall problem is the same across this series: we are trying to have a sentence encoder which works well across tasks, languages and domains.

<br/>
<br/>

<span style="font-size:15px;"><b>Synopsis of the previous episode</b></span>
<br/>

  ⏮️    We have a sentence encoder which works well in several languages and several domains but it was only tested on the 16 languages it was trained on.

<br/>
<br/>
<span style="font-size:15px;"><b>Problem</b></span>
<br/>
  ❔    Can we train a sentence encoder which is general with respect to input language, script and NLP task we use it for?

<br/>
<br/>
<span style="font-size:15px;"><b>What is the solution proposed?</b></span>
<br/>

<b><i>Model</i></b>
<br/>

🔬The model is a sequence to sequence model trained for translation. 
<br/>
🔬 Strong sentence representation is obtained by passing <i>only</i> the overall sentence encoding to the decoder (unlike the usual translation pipeline where the decoder has access to the encoding of each token in the sentence).
<br/>
🔬 Each input sequence was jointly translated only into two languages (en, es) and not all 93 languages for two reasons -- no need for  93-way parallel corpus, no need to train 93 decoders.
<br/>
🔬 The architecture is a a biLSTM preceded by BPE encodings. 
<br/>
<br/>

<b><i>Data</i></b>
<br/>
💽 A set 223 million parallel sentences --  a combination of United Nations, OpenSubs etc.
<br/>
💽 The 93 languages come from more than 30 language families and written in 28 different scripts.
<br/>
<br/>

<span style="font-size:15px;"><b>Results</b></span>
<br/>
⭐ Strong performance on XNLI (14 languages within the 93), MLDOC (document classification, 7 languages), BUCC (finding parallel sentences in large corpora);
<br/>
⭐ NEW dataset: Similarity search corpus in 112 languages; obtained to assess the model's performance on out-of-training-dataset languages.
<br/>
⭐ The dataset shows that the model generalizes well to out of training data languages
<br/>
⭐ In an ablation study, they show that joint training on 93 languages is useful. 
<br/>
<br/>