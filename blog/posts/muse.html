---
layout: page
title: Multilingual Universal Sentence Encoder for Semantic Retrieval
comments: false
tags: [blog, Evgeniia-Razumovskaia, research, NLP, dialogue systems, welcome, phd-life, phd]
---


Yes, it's a weird <b>retro</b>spective which starts with a paper of year 2019. I guess it shows how quickly the field is moving. 
<br/>
<br/>

<b>Preface</b>
<br/>

  🔲    Existing large language models such as BERT obtain state-of-the-art performance on many natural language understanding tasks. They are pretrained to create contextual word embeddings and sentence embeddings via masked language modelling and next sentence prediction. 
<br/>
  🔲    In monolingual settings they are very effective for both word-level and sentence level tasks. Multilingual representations of sentences were not as effective in cross-lingual settings. 

<br/>
<br/>
<b>Problem</b>
<br/>
  🌎    For multilingual retrieval tasks, we require goood universal sentence encoder. By universal here I mean one that can capture semantics correctly in multiple domains in several languages. 
<br/>
  ❔    How can we train such a universal encoder? Is there a possibility to make it more lightweight than its predecessors? 

<br/>
<br/>
<b>What is the solution proposed?</b>
<br/>

<b><i>Model</i></b>
<br/>
<img src="/../../images/posts/mUSE/model_encoder.png" style="max-width:65%; margin-left:20px;"/>
<br/>

🔬The model is a <i>dual encoder</i> model: one side encodes the query, e.g., the question in the QA task, and the other side encodes all possible candidates, e.g., all possible responses in the QA tasks. 
<br/>
🔬The model computes a similarity metric between the query encoding and the response encodings. The output of the model is the response most similar to the query. 
<br/>
🔬 2 architectures were tried as the encoder: Convolutional Neural Network for parameter efficient network and Transformer encoder for higher accuracy but more resourceful.
<br/>

<b><i>Data</i></b>
<br/>