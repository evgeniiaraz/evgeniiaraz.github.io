---
layout: page
title: Multilingual Universal Sentence Encoder for Semantic Retrieval
comments: false
tags: [blog, Evgeniia-Razumovskaia, research, NLP, dialogue systems, welcome, phd-life, phd]
---


Yes, it's a weird <b>retro</b>spective which starts with a paper of year 2019. I guess it shows how quickly the field is moving. 
<br/>
<br/>

<b>Preface</b>
<br/>

  üî≤    Existing large language models such as BERT obtain state-of-the-art performance on many natural language understanding tasks. They are pretrained to create contextual word embeddings and sentence embeddings via masked language modelling and next sentence prediction. 
<br/>
  üî≤    In monolingual settings they are very effective for both word-level and sentence level tasks. Multilingual representations of sentences were not as effective in cross-lingual settings. 

<br/>
<br/>
<b>Problem</b>
<br/>
For multilingual retrieval tasks, we require goood universal sentence encoder. By universal here I mean one that can capture semantics correctly in multiple domains in several languages. 
<br/>
  ‚ùî    How can we train such a universal encoder? Is there a possibility to make it more lightweight than its predecessors? 

<br/>
<br/>
<b>What is the solution proposed?</b>
<br/>

