---
layout: page
title: Multilingual Universal Sentence Encoder for Semantic Retrieval
comments: false
tags: [blog, Evgeniia-Razumovskaia, research, NLP, dialogue systems, welcome, phd-life, phd]
---

Yes, it's a weird <b>retro</b>spective which starts with a paper of year 2019. I guess it shows how quickly the field is moving. 
<br/>
<br/>

<span style="font-size:14px;">
This is a post with notes on the paper
    <b><a href="https://arxiv.org/pdf/1907.04307.pdf">
        Multilingual Universal Sentence Encoder for Semantic Retrieval
    </a></b>
    by Yinfei Yang, Daniel Cer et al.
</span>


<span style="font-size:15px;"><b>Preface</b></span>
<br/>

  üî≤    Existing large language models such as BERT obtain state-of-the-art performance on many natural language understanding tasks. They are pretrained to create contextual word embeddings and sentence embeddings via masked language modelling and next sentence prediction. 
<br/>
  üî≤    In monolingual settings they are very effective for both word-level and sentence level tasks. Multilingual representations of sentences were not as effective in cross-lingual settings. 

<br/>
<br/>
<span style="font-size:15px;"><b>Problem</b></span>
<br/>
  üåé    For multilingual retrieval tasks, we require goood universal sentence encoder. By universal here I mean one that can capture semantics correctly in multiple domains in several languages. 
<br/>
  ‚ùî    How can we train such a universal encoder? Is there a possibility to make it more lightweight than its predecessors? 

<br/>
<br/>
<span style="font-size:15px;"><b>What is the solution proposed?</b></span>
<br/>

<b><i>Model</i></b>
<br/>
<img src="/../../images/posts/mUSE/model_encoder.png" style="max-width:65%; margin-left:20px;"/>
<br/>

üî¨The model is a <i>dual encoder</i> model: one side encodes the query, e.g., the question in the QA task, and the other side encodes all possible candidates, e.g., all possible responses in the QA tasks. 
<br/>
üî¨The model computes a similarity metric between the query encoding and the response encodings. The output of the model is the response most similar to the query. 
<br/>
üî¨ 2 architectures were tried as the encoder: Convolutional Neural Network for parameter efficient network and Transformer encoder for higher accuracy but more resourceful.
<br/>

<b><i>Data</i></b>
<br/>

For the languages covered, the data includes:
<ul>
<li>question answer pairs mined from Reddit, Stack overflow etc.</li>
<li>translation pairs mined from the Web</li>
<li><a href="https://nlp.stanford.edu/projects/snli/">Stanford NLI corpus</a></li>
</ul>

To make the amount of data equal across 16 languages, the data missing from one was translated into the other ones using Google Translate service. 


<br/>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<br/>
Languages covered:
<table class="tg">
<thead>
  <tr>
    <th class="tg-0lax">English (en)</th>
    <th class="tg-0lax">French (fr)</th>
    <th class="tg-0lax">Dutch (nl)</th>
    <th class="tg-0lax">Thai (th)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">Arabic (ar)</td>
    <td class="tg-0lax">Italian (it)</td>
    <td class="tg-0lax">Portuguese (pt)</td>
    <td class="tg-0lax">Turkish (tr)</td>
  </tr>
  <tr>
    <td class="tg-0lax">German (de)</td>
    <td class="tg-0lax">Japanese (ja)</td>
    <td class="tg-0lax">Polish (pl)</td>
    <td class="tg-0lax">Chinese (zh; zh-tw)</td>
  </tr>
  <tr>
    <td class="tg-0lax">Spanish (es)</td>
    <td class="tg-0lax">Korean (ko)</td>
    <td class="tg-0lax">Russian (ru)</td>
    <td class="tg-0lax"></td>
  </tr>
</tbody>
</table>

<span style="font-size:15px;"><b>Results</b></span>
<br/>

‚≠ê Performs well across 16 languages; 
<br/>
‚≠ê Transfers the knowledge well to unseen retrieval tasks, e.g., sentence classification within SentEval.
<br/>
‚≠ê CNN model demonstrates worse performance than Transformer model but is far more resource efficient.
<br/>
<br/>

<span style="font-size:15px;"><b>And?</b></span>
<br/>
üí≠ We have a model which works well on 16 languages coming from 7 different language families but it hasn't been in transfer to other languages not covered by training data has not been tested. The set of languages is large but they are mostly high resource laguages. 
<br/>
üí≠ CNN model is far more resource efficient which makes it a more appealing option for industry, especially when you need to host the model on device.